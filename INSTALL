
1. Package Prerequs

Last configured with

ii  elasticsearch                             0.20.5                                    all          Open Source, Distributed, RESTful Search Engine
ii  mongodb-10gen                             2.4.1                                     amd64        An object/document-oriented database

2. Config Prereqs

Mongo needs to be configured with replica set enabled.. Info here: http://docs.mongodb.org/manual/tutorial/deploy-replica-set/ but in essence add

-----
# in replica set configuration, specify the name of the replica set
replSet = rs0
-----

to /etc/mongodb.conf

Then run mongo command line and execute

rs.initiate()

To start replication. rs.conf() and other commands can be used to dump connfig.

Don't forget to bounce mongo

https://github.com/richardwilly98/elasticsearch-river-mongodb
https://github.com/richardwilly98/elasticsearch-river-mongodb/wiki


2.1. Required ES plugins

/usr/share/elasticsearch/bin/plugin -install elasticsearch/elasticsearch-mapper-attachments/1.7.0
/usr/share/elasticsearch/bin/plugin -url https://github.com/downloads/richardwilly98/elasticsearch-river-mongodb/elasticsearch-river-mongodb-1.6.4.zip -install river-mongodb
/usr/share/elasticsearch/bin/plugin -install mobz/elasticsearch-head



3. Schemas and collections

create database localchatterprod default charset UTF8 default collate utf8-bin;
grant all on localchatterprod.* to 'localchatter'@'localhost' identified by 'localchatter';
grant all on localchatterprod.* to 'localchatter'@'localhost.localdomain' identified by 'localchatter';
grant all on localchatterprod.* to 'localchatter'@'%' identified by 'localchatter';

Initial ES setup

Run

scripts/clear_down.sh

under this directory. This will clear down any existing mongo databases,so be warned!! It will also configure ES collections and the mongod rivier


4. Data

To pre-populate the various source collections, you can load the json dumps. 

For me thats

mongoimport --db osgazcache --collection entries < ~/Dropbox/lc/osgazcache_dump.json 
mongoimport --db ofsted_crawl_db --collection ofsted < ~/Dropbox/lc/ofsted.json

5. config

~/.grails/api-config.groovy

-----
sysusers = [
  [
    name:'admin',
    pass:'defadmpassword',
    display:'Admin',
    email:'admin@localhost',
    roles: [ 'ROLE_USER', 'ROLE_ADMIN']
  ] 
]


es {
  cluster='localchatter'
}
-----

~/.grails/localchatter-config.groovy

-----
sysusers = [
  [
    name:'admin',
    pass:'__l1v3lcch4hg3m3',
    display:'Admin',
    email:'admin@localhost',
    roles: [ 'ROLE_USER', 'ROLE_ADMIN']
  ] 
]


es {
  cluster='localchatter'
}

frontend='http://www.localchatter.info/localchatter'
-----


6. Test run... To push the ofsted data loaded in the dump above into the database, you can start the api project then
cd into scraping/ofsted
and run
./upload.groovy



7. Test

After the above, you should be able to query using

http://localhost:9200/localchatter/_search?q=*
